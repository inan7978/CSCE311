Time: 

The time it takes program to load the file is ultimately in linear proportion to the size of the file. The massive files provided, which are extreme cases, took 21 minutes to load for the 6.5M file and 18 minutes for the Anna Karenina file. Once the word was entered the program had the lines found, saved sorted, and outputed instantly.

Different IPC methods:

	There are many. Pipes, named pipes (FIFO), message queuing, semaphores, shared memory, and sockets. In terms of performance, the best method to use on a local machine is no doubt shared memory. Granted shared memory is more complex to implement, with its signaling mechanisms and locks, but sending data between processes happens WITHOUT the need to copy it to an "intermediary". Whenever one process write something to the memory, the other process just reads the same bit of memory, hence, the information is already there, ready to be analyzed and manipulated. I am not familiar with implementing shared memory communications, but I will continue my research into implementing them and making my program perform many times faster. Pipes allow flow of data in one direction only. It is analogous to simplex systems, for example, a keyboard. For simplicity reasons a socketpair is a version of a socket and it is easy to implement. This only works between parent and child. Message Queuing allows messages to be passed between process using a single queue and/or a several messages queues. The messages are coordinated using an API and managed by the system kernel. Semaphores are used to prevent race conditions and are simply integers with a value of 1 or 0. FIFO is a named pipe which can be used in processes that dont contain a shared process origin.

Map-Reduce:

	Map reduce is a model for processing big data sets with a parallel and distributed algorithm on a cluster.It is essentially a program which determines which computers in the cluster do what in the task. An input is split up and is divided between many computers. For simplicity's sake the easiest Map-Reduce methods deal with a handful of computers that are all exactly the same. In the Map phase, a task is split up into a smaller task and is sent to a machine for processing. Once each computer does it's part, all of the outputs of the machines on the cluster are reduced down to a single value (the reduce stage) and sent back as the result of the computation. For larger clusters with varying types of hardware, more sophisticated algorithms are used that take into account what each computer on the cluster does best to ensure speed. This is very important in industry as it allows massive amonts of data to be computed while at the same time reducing cost. This is also a very interesting topic that I will continue my research on. It will also make use of old computers...

Hadoop:

	Hadoop is pretty much the backbone of big data operations. Released in 2005 by the Apache Software Doundation, there are 4 Modules of Hadoop, each carrying out a task that a computer system designed for big data analytics needs, hence, backbone. The two most important are the Distributed File System and the MapReduce. The Distributed File System allows data to be stored in an accessible format across a large number of linked storage devices. MapReduce, in this sense, provides the tools for accessing the data. YARN is another module. YARN manages the resources of the source system. This system is where data is stored and the analysi preformed. THe final module is Hadoop Common. This module provides the necessary tools the computer needs to read data stored under hadoop file system. Hadoop is important in industry as it allows companies to add or modify thier data systems as thier needs change. Not only this, but it allows it to be done cheaply and with readily available hardware. 

